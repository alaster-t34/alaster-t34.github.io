---
title: 對，我還是不死心！
published: 2025-01-14
description: ''
image: './wuhei.jpg'
tags: [AI, Live2D]
category: 'live2d'
draft: false 
lang: ''
---

這一篇是續著上一篇的《FaiREE-AIイラストか》而寫的，因爲發現上一篇那個幾乎沒啥用，並不能解決我的問題（笑）。衆所周知，我會畫立繪，但是！我太懶了，而且因爲時間問題（我的時間永遠是斷斷續續的），就打算讓AI製作live2d模型。<br>

- 首先不得不承認的是：直接通过代码生成 Live2D 模型是相对复杂的<br>
因为 Live2D 模型的创建涉及到图像处理、骨骼动画和物理引擎等多个方面，而这些技术都是相对复杂的。因此，直接通过代码生成 Live2D 模型并不现实。
- 但是：理论上是可以通过编程生成或自动化 Live2D 模型的部分步骤<br>
這些部分步驟，比如图像处理、骨骼动画和物理引擎，理论上是可以自动化完成的。比如，图像处理部分，可以利用计算机视觉技术，对人物的照片进行分析，提取出人物的轮廓、皮肤、眼睛、嘴巴等特征，并将这些特征转化为 Live2D 模型的贴图。骨骼动画部分，可以利用机器学习算法，对人物的动作进行分析，并生成 Live2D 模型的动画。物理引擎部分，可以利用物理模拟技术，模拟人物的运动，并生成 Live2D 模型的物理效果。
- 因此：理论上是可以自动化生成 Live2D 模型的<br>

### 關於 自动化图像切割与建模
尽管 Live2D 的建模和动画主要依赖于手动操作和设计，但可以使用一些代码工具（如 Python 脚本）来辅助自动化某些部分（應該），比如：在图像的切割和图层划分上。<br>
步驟的話：<br>
图像处理：使用 Python 的 OpenCV 库对图像进行处理，提取出人物的轮廓、皮肤、眼睛、嘴巴等特征。（或者PIL库）<br>
导出 Live2D 数据：理论上，可以通过代码将这些分割后的图层和骨骼设定转换为 Live2D 模型所需的格式（如 .moc3、.model3.json 等）。（然而，这部分需要深刻了解 Live2D 的文件格式和动画规则，是一个非常复杂的过程。）<br>

### 關於 自动化骨骼动画生成
Live2D 的动画生成主要依赖于手动操作，但可以使用一些代码工具（如 Python 脚本）来辅助自动化某些部分（應該），比如：在动作的分析和动画生成上。<br>
步驟的話：<br>
动作分析：使用 Python 的 OpenCV 库对视频进行处理，提取出人物的动作特征。（或者用其他的动作识别库）<br>
导出 Live2D 数据：理论上，可以通过代码将动作特征转换为 Live2D 模型的动画文件（如 .motion3.json 等）。（然而，这部分需要深刻了解 Live2D 的文件格式和动画规则）<br>

### 關於 自动化物理引擎生成
Live2D 的物理引擎生成主要依赖于手动操作，但可以使用一些代码工具（如 Python 脚本）来辅助自动化某些部分（~~應該~~），比如：在人物的运动模拟上。<br>
步驟的話：<br>
运动模拟：使用 Python 的 PyBullet 库进行物理模拟，模拟人物的运动。（或者用其他的物理引擎库~~?~~）<br>
导出 Live2D 数据：理论上，可以通过代码将物理模拟结果转换为 Live2D 模型的物理文件（如 .physics3.json 等）。（然而，这部分需要深刻了解 Live2D 的文件格式和物理规则）<br>

### ~~結論~~
~~Live2D 模型的自动化生成，理论上是可以实现的，但還屬於起步階段。<br>~~
~~并且不是一個大一統的程序可以解決的（就目前而言因爲要寫這莫一個程序是十分複雜的，畢竟是生成系統，文件而非一張動畫）~~

## 難道這就結束了嗎？終歸是徒勞嗎？不，一定是遺漏了什麽······

因爲是要構建一個live2d模型文件夾，所以，根據下載的那麽多文件來看，首先是拆分出來的圖片，然後再來是動畫文件，最後是物理文件。那麽對這三個都進行自動化生成，那麽就需要有一個自動化工具，這個工具可以將圖片切割、動畫生成、物理模擬，然後合併成一個live2d模型文件夾。

那麽，這個工具應該要有什麽功能呢？

- 圖片切割：將圖片切割成可以被Live2D模型使用的格式，例如.moc3、.model3.json等。
- 動畫生成：將動作轉換成Live2D模型的動畫文件，例如.motion3.json等。
- 物理模擬：將物理模擬結果轉換成Live2D模型的物理文件，例如.physics3.json等。
- 合併文件：將圖片、動畫、物理文件合併成一個Live2D模型文件夾。

#### 圖像分割技術
* 圖像分割技術。通過使用深度神經網絡（如捲積神經網絡，CNN）。有如下幾種辦法。
 * α.深度學習模型（如 U-Net、Mask R-CNN）<br>
利用模型進行語義分割，即自主識別圖像的不同區域（如頭髮、眼睛、嘴巴等）。這個可以將關鍵部位提取至單獨圖層。
 * β.工具和框架<br>
  * DeepLabV3：Google提供的一個深度學習模型，用於圖像分割；
  * RunwayML：一個圖像分割工具，可以將圖像分割成不同類別的區域。
  Stable Diffusion 或 MidJourney（生成图像辅助工具）：~~虽然这些工具主要用于生成图像，但也有潜力帮助生成分层图像，或通过风格转换将图像转化为适合 Live2D 的风格。~~
 * ~~γ.懶人的現成AI工具~~<br>
~~AI Studio：[^提供Live2D模型的生成，可以直接上傳照片，生成Live2D模型文件夾。]~~:（你在想什麽?真有這莫好的工具？那我早用了！還寫個damn）
* ~~自動化拆分工具（？有這玩意？我不知道啊）~~
 ~~Clip Studio Paint：这款软件允许你为角色插图设计分层和分割，并且它提供了一些智能辅助工具来帮助简化分割过程。虽然它本身不是 AI 驱动，但通过其强大的图层功能，可以加速手动拆分的过程。（GPT説的）~~
 我的感覺就是：自己寫一個的了。。。

睏了，睡會兒。

（唔，睡了一晚上。。。）<br>


於是是可以用ai來拆分，但是要如何生成動畫文件、物理文件方面。。。
<br>
先讓我歸籠下文件組成：<br>

* \1. .moc3文件：
     Live2D模型的骨架文件，包含了模型的骨骼、頂點、材質、材質參數等信息。它是經過Live2D Cubism編輯器處理後生成的，所以說是模型的“骨架”。
* \2. .model3.json文件：
     這個文件用來描述模型的基本設定、繪製層級、動畫設定等。它是和.moc3文件配合使用的，通常包含了模型的配置、材質、物理效果等參數。
* \3. .motion3.json文件：
     Live2D模型的动画文件，包含了模型的各个部件的动画轉換信息。這些動作可以是表情變化、眼睛眨動、頭部旋轉等。這些動畫可以被觸發並進行時間控制。
* \4. .physics3.json文件：
     Live2D模型的物理文件，包含了模型的物理效果信息。
* \5. 材質文件（通常是.png、.jpg等圖片格式）
     模型的各個部件（如頭髮、眼睛、衣服等）會使用不同的圖片作為材質，這些圖片需要被指定到.model3.json中相應的部位。(這裏的應該是拆分后的立繪)
* \6. .pose.json文件
     Live2D模型的初始姿勢文件，定義了模型的初始姿勢。這些文件存儲了模型的靜態設定，並允許設置初始的眼睛、嘴巴等位置。
* \7.Live2D Cubism SDK
     在開發中，Live2D Cubism SDK會用來將所有的文件和設定合併，並導出最終的模型，以便在應用程序或遊戲中使用。
* \8.音效和配音文件（可選） 
     模型的聲音可以被指定到.model3.json文件中，並且可以被觸發。

在瞭解到這個后，我的打算是看看別人的那些模型文件夾，結果呢。。。令人費解的事情發生了。。。<br>

我找到了用于在网页上展示的《少前》的79式模型文件，結果呢，裏面只有.moc3，.mtn，.model3.json和拆分后的立繪。其他的文件，直接没有？！<br>
而且，79式的模型文件夾裏的model.json與我從b站上下載的模型文件夾裏的model.json不太一樣，裏面有很多的差異。。。<br>
故我的猜想是：79式的模型因爲是用於網頁的，所以省略了其他的文件；而B站上的模型是爲了適應其他的用途，所以較爲齊全。
(不過，79式的model.moc文件不能用live2dcubism打開，屮)

## ~~有一説一，這篇文章寫得真是亂七八糟的~~



  


