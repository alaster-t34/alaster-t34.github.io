---
title: 關於·訓練ai讓其對原畫立繪進行拆分(圖像分割）的步驟的一些基层想法
published: 2025-01-16
description: ''
image: './chaifen.jpg'
tags: [AI, Live2D]
category: 'live2d'
draft: false 
lang: ''
---
## 我是這樣想的，這個過程應該可以通過圖像分割和深度學習來實現。
具體步驟大概如下：
### 1. 收集數據集
需要收集帶有標註的數據集，***每張圖片對應著其拆分後的部件***

***數據集需要包括***：
* 原畫立繪圖片。
* 每個部件的掩碼圖（mask），標註出每個部件的邊界和區域，這是模型學習拆分的關鍵。
#### 數據集的質量對訓練效果至關重要，因此需要標註準確，且數據量足夠多（通常需要數千張圖片及其對應的標註）。
<br>

* #### 1.關於·手動標註數據
直接且準確，***~~（但時間成本高。<!--可以想象一下，手動，幾千幾萬張圖片，玩個damn！-->）~~***<br>
本人不建議，<!--除非你雇了一堆人手來做這件事。-->大概步驟是這樣：
* 1. 使用標註工具來手動標註每個部件。這些工具允許你在每張圖片上創建掩碼（mask），標註每個部件。
* 2. 將每個部件的區域保存為單獨的圖層，通常是二進制掩碼圖（例如黑色背景，白色部分代表部件）
能用：
* 1.LabelMe
* 2.VGG Image Annotator (VIA)
* 3.LabelImg
* 4.CVAT
* 5.RectLabel
<!--不想用。。。有的還要money,我自己也不打算用這個，因爲我就我一個人，手動太麻煩了。-->

<br>

* #### 2.關於·現有的標註數據集
（~~使用現有的標註數據集，可以節省時間，但質量。。。）~~
如果已有的標註數據集包含了與原畫立繪拆分相關的圖像，可以使用這些數據集來加速訓練。雖然這樣的數據集不常見，但一些公共數據集可能會有所幫助（應該吧）
* 1.COCO Dataset
* 2.Pascal VOC Dataset
* 3.ImageNet Dataset
* 4.Open Images Dataset
* 5.ADE20K
<!--能不能用我不確定-->

<br>

* #### 3.關於·半自動標註
~~（手動標註的時間成本高，因此可以考慮使用半自動的工具來標註數據。但質量畢竟。。。）~~
大概步骤步骤如下：
* 使用已有的深度學習模型（如 U-Net 或 DeepLab）對原畫立繪圖片進行初步分割，生成初步的部件掩碼。
* 這些掩碼雖然可能不完美，但可以作為標註的基礎，人工進行修正。
* 對每個部件進行細微調整，保證標註的準確性。

<br>

* #### 4.關於·使用圖層分離工具
~~（这个似乎是比较简单的了）~~
直接提取每個部件的圖層，並將其作為掩碼圖。（大霧）
<br>


> #### 诶！这时候就有人要说了：能不能不用掩码图？
><br>
>> 我还真想了这事，但是这玩意会导致模型的复杂度增加<br>
>> 我想的是，畢竟沒有現成的掩碼圖，故需要自己生成（或是用模型生成），但是，這樣生成的模型它的局部性太强，不具有全局概括性，可能导致模型欠擬合。而且，操作有些麻煩。
>><br>
>> 先來説説複雜度增加一事：

>> * 首先，數據標記的複雜性<br>
>> 在無掩碼圖的情況下，模型無法依賴直接的注釋信息（像素級標簽），無掩碼圖將會導致工作更爲複雜。<br>
>>>  * 手動標記的工作量大<br>
>>>  * 手動標記的標注精度低<br>
>>>  * 手動標記的標注成本高<br>

>> * 其次，模型訓練的複雜度<br>
>>>  * ***學習目標不明確***· 模型需要依賴間接的提示或約束條件（例如局部標注、特徵信息等等）來理解圖像的結構和内容，而不是像傳統方法那樣有像素級標簽。而且，損失函數將難以優化與設計。
>>>  * ***特徵學習難度大***· 模型要從整體特徵中去推斷出不同部分的邊界與關係，那麽就要讓模型不僅要學習到每個區域的特徵，還要學習如何區分這些區域。（你可以想象一下，這個模型的學習能力要多强···）
當然還有一系列的問題，不多説了。
~~（罪加一等哈~）~~
<!--但是非要用的話，也行。哪位能人搞出來了，私信我的X。orz-->

<br>

### 2. 訓練模型
訓練模型的目的是讓模型學習到如何分割出每個部件，並且學習到如何合成出完整的立繪。
訓練模型的目標是生成一個模型，可以根據輸入的圖片，輸出每個部件的掩碼圖（mask）和合成圖片。
訓練模型的流程大概如下：
* 選擇模型架構，例如：U-Net、SegNet、FPN、Mask R-CNN、DeepLab等。
* 數據預處理：
            1、圖像縮放與裁剪：將所有圖像統一大小，避免過大的圖像影響訓練效率。<br>
            2、正規化：對圖像進行正規化處理，使其像素值處於 [0, 1] 或 [-1, 1] 的範圍內。<br>
            3、數據增強：對圖片進行隨機裁剪、旋轉、翻轉等操作，增加訓練數據的多樣性，提升模型的泛化能力。
#### 訓練過程：            
* 定義損失函數，例如：交叉熵、Dice Loss、Dice coefficient損失等。
* 定義優化器，例如：Adam、SGD等。
* 訓練模型，使用訓練數據進行模型訓練，根據每次訓練的損失函數來調整模型參數。<br>
**訓練過程中，模型將學習如何將每個像素點分類為不同的部件（如髮型、衣服等），並學會拆分圖像。**

### 3.後處理
訓練完模型後，進行後處理來優化結果：
* 邊緣檢測：檢測模型輸出的掩碼圖（mask）是否有缺陷，如：邊緣太粗、邊緣太細、邊緣太尖等。
* 邊緣修正：對模型輸出的掩碼圖（mask）進行邊緣修正，如：用多邊形裁切、用模糊處理等。
* 小區域去除：去除一些不必要的小區域，避免分割過程中出現噪聲。

### 4.測評
* IoU：用來衡量預測結果和真實標註之間的重疊度。
* Dice係數：用來評估分割結果的準確性，特別是對於類別不平衡的問題。

### 5. 將模型部署到ai上
將訓練好的模型部署到ai上，讓ai可以根據輸入的圖片，輸出每個部件的掩碼圖（mask）和合成圖片。
部署模型的流程大概如下：
* 將訓練好的模型轉換為可部署的格式，例如：ONNX、TensorRT等。
* 將模型部署到ai上，例如：Tensorflow、PyTorch、Caffe、MXNet等。
* 將模型的輸入圖片轉換為模型可以接受的格式，例如：JPG、PNG、BMP等。
* 將模型的輸出掩碼圖（mask）和合成圖片轉換為可視化的格式，例如：JPG、PNG、BMP等。
* 將模型的輸出結果呈現。

### 6. 持續優化
用pytorch，tensorflow等框架，對模型進行持續的訓練，提升模型的性能。

